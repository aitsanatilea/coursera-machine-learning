---
title: "Practical Machine Learning. Course Project. Prediction Assignment Writeup"
author: "Anastasiia Alieksieienko"
date: "7/9/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment = "")
```

```{r setworkdir, echo=FALSE}
setwd("/Users/anastasia/Study/course8/assignment")
```

## Overview 
In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. 

The original data was preprocessed to remove NA and empty columns using dplyr package and imputed some of the missing values in the remaining ones. Then 6 models (lda, lda2, rf, gbm, knn and kknn) were fitted with cross-validation using caret library. Comparison of the models on the validation subset showed that Random forest model has the highest accuracy. The selected model then was applied to the test data set.


## Loading and Preprocessing the Data
The following code loads csv files with training and testing data to the current working directory and then reads them in. 
```{r load_data, cache=TRUE}
trainUrl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainUrl,destfile=paste0(getwd(),"/pml-training.csv"), method = "curl")
download.file(testUrl,destfile=paste0(getwd(),"/pml-testing.csv"), method = "curl")
pmlTrain<-read.csv("pml-training.csv", stringsAsFactors = FALSE, na.strings=c("NA", "", "#DIV/0!"))
pmlTest<-read.csv("pml-testing.csv", stringsAsFactors = FALSE, na.strings=c("NA", "", "#DIV/0!"))
```
The training data set contains of `r nrow(pmlTrain)` observations on `r ncol(pmlTrain)` variables.
There are a lot of columns containing mostly NAs or "" in the training data set, so I've removed them both from training and testing data sets. I've also excluded variables 1:7 that are not relevant for predicting the way the excercise was done.
```{r removing_na, cache=TRUE}
library(dplyr)
# Removing columns with NA values
csum<-colSums(is.na(pmlTrain))
nanames<-names(csum[csum>19000])
pmlTrain2<-select(pmlTrain, -c(nanames))
pmlTesting<-select(pmlTest, -c(nanames))
# Removing irrelevant columns
pmlTrain2<-pmlTrain2[,-(1:7)]
pmlTesting<-pmlTesting[,-(1:7)]
```
```{r checking_na, echo=FALSE}
# Checking for NA both in train and test data sets
#which(!complete.cases(pmlTrain2)); which(!complete.cases(pmlTest2))
```
Next step is to check the remaining columns for zero/near-zero values and remove those if there are any. 
```{r removing_nzv, results="hide", cache=TRUE}
library(caret)
nzval<-nearZeroVar(pmlTrain2, saveMetrics = TRUE)
nzval 
```
As we can see  - all are false, so there is nothing to remove.
```{r result_removing_nzv, ref.label='removing_nzv', eval=TRUE, echo=FALSE}
```
After the procedure the number of variables decreased to `r ncol(pmlTrain2)`. Since there are a lot of observations in training set we can split it in two subsets: training (75%) and validation (25%) in order to find out what model is better before applying it to the test set. 
```{r}
pmlTrain2$classe<-as.factor(pmlTrain2$classe)
```

```{r creating_subsets}
library(caret)
set.seed(123456)
inTrain<-createDataPartition(y=pmlTrain2$classe, p=0.75, list = FALSE)
pmlValidation<-pmlTrain2[-inTrain,]
pmlTraining<-pmlTrain2[inTrain,]
```

## Fitting models
Since many models utilize random numbers during the phase where parameters are estimated and to ensure that the same resamples are used between calls to train we'll use *set.seed* prior to every call to *train* function. We will fit 6 models: 

* Linear Discriminant Analysis - lda2
* Random Forest - rf
* Generalized Boosted Regression Model - gbm
* Support Vector Machines - svm
* Two types of K-Nearest Neighbours (regular and weighted) - knn, kknn  

For every model the 3-fold **Cross-Validation** is used by applying *trControl=trainControl(method="cv", number=3)*. Then we'll predict *classe* variable for Validation data set and build confusion matrices.


### Linear Discriminant Analysis

```{r model_lda2, cache=TRUE}
set.seed(123456)
fitlda2<-train(classe~., data=pmlTraining, method="lda2", preProcess="knnImpute",
              trControl=trainControl(method="cv", number=3))
fitlda2
```
```{r}
vPredict<-predict(fitlda2, pmlValidation)
cmlda2<-confusionMatrix(pmlValidation$classe,vPredict); cmlda2
```

### Random Forests

```{r model_rf, cache=TRUE}
set.seed(123456)
fitrf<-train(classe~., data=pmlTraining, method="rf", preProcess="knnImpute",
             trControl=trainControl(method="cv", number=3))
fitrf
```
```{r}
vPredict<-predict(fitrf, pmlValidation)
cmrf<-confusionMatrix(pmlValidation$classe,vPredict); cmrf
```

### Generalized Boosted Regression Model  

```{r model_gbm, cache=TRUE}
set.seed(123456)
fitgbm<-train(classe~., data=pmlTraining, method="gbm", preProcess="knnImpute",
              trControl=trainControl(method="cv", number=3), verbose=FALSE)
fitgbm
```
```{r}
vPredict<-predict(fitgbm, pmlValidation)
cmgbm<-confusionMatrix(pmlValidation$classe,vPredict); cmgbm
```

### Support Vector Machines  

SVM is one of the most widely-used and robust classifiers. Not only can it efficiently classify linear decision boundaries, but it can also classify non-linear boundaries and solve linearly inseparable problems. As we can see it's little less accurate than rf and gbm, but much more precise than lda.

```{r model_svm, cache=TRUE}
library(e1071)
set.seed(123456)
fitsvm<-svm(classe~., data=pmlTraining)
fitsvm
```
```{r}
vPredict<-predict(fitsvm, pmlValidation)
cmsvm<-confusionMatrix(pmlValidation$classe,vPredict); cmsvm
```

### K-Nearest Neighbor Classifier

```{r model_knn, cache=TRUE}
set.seed(123456)
fitknn<-train(classe~., data=pmlTraining, method="knn", preProcess="knnImpute",
              trControl=trainControl(method="cv", number=3))
fitknn
```
```{r}
vPredict<-predict(fitknn, pmlValidation)
cmknn<-confusionMatrix(pmlValidation$classe,vPredict); cmknn
```

### Weighted k-Nearest Neighbor Classifier

Performs k-nearest neighbor classification: for each row of the test set, the k nearest training set vectors (according to Minkowski distance) are found, and the classification is done via the maximum of summed kernel densities.   
```{r model_kknn, cache=TRUE}
library(kknn)
set.seed(123456)
fitkknn<-train(classe~., data=pmlTraining, method="kknn", preProcess="knnImpute",
              trControl=trainControl(method="cv", number=3))
fitkknn
```
```{r}
vPredict<-predict(fitkknn, pmlValidation)
cmkknn<-confusionMatrix(pmlValidation$classe,vPredict); cmkknn
```

```{r multiplot, echo=FALSE}
# Multiple plot function 
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }
 if (numPlots==1) {
    print(plots[[1]])
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

### Plots of final models

Meaningful plots can be built for 2 models only. For random forest we can see that error decreases with increaing of the number of trees built. Plot for kknn model gives us information about the quality of the classification based on the number of neighbors.
```{r plots_models, fig.align='center', fig.width=12, cache=FALSE}
par(mfrow=c(1,2),mar=c(5,4,2,2))
plot(fitrf$finalModel, main="RF")
plot(fitkknn$finalModel, main="KKNN") 
```

### Plots for tuning parameters

The following plots show how the accuracy changes while the parameters of the models are tuned. The model parameters are selected based on the accuracy value.

```{r plots, fig.align='center', fig.width=12, cache=FALSE}
p1<-ggplot(fitlda2) + labs(title="lda2") + theme_bw()
p2<-ggplot(fitrf) + labs(title="rf") + theme_bw()
p3<-ggplot(fitgbm) + labs(title="gbm") + theme_bw()
p4<-ggplot(fitknn) + labs(title="knn") + theme_bw()
p5<-ggplot(fitkknn) + labs(title="kknn") + theme_bw()
multiplot(p1, p4, p2, p5, p3, cols=3)
```

### Accuracy comparison

Random forest and KKNN model have the best accuracy, but RF is even more precise, so I'll choose it for predicting for the test data set.
```{r accuracy_comparison}
accuracyDF<-data.frame(Model=c("lda2", "rf", "gbm", "svm","knn", "kknn"),
                       Accuracy=c(cmlda2$overall[1], cmrf$overall[1], cmgbm$overall[1],
                                  cmsvm$overall[1], cmknn$overall[1], cmkknn$overall[1]))
accuracyDF
```

## Predicting for Test Data

Let's predict the style of doing exercises for the test data set using trained Random Forest model with Cross Validation.

```{r predict_test}
testPredict<-predict(fitrf, pmlTesting)
testPredict
```

## Conclusion
  
#### Random forest model gives the highest accuracy - `r cmrf$overall[1]` - on the validation data, so it was used to predict the outcomes for the test data set.





